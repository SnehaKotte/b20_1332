{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNHrbuQN0k+oQmRj8kp0+Xb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SnehaKotte/b20_1332/blob/main/iEEE2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4-_aLlIYFs8",
        "outputId": "671bd98b-619e-4bb5-8dcb-794391c8458b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset... /content/data_set.csv\n",
            "Dataset shape: (3547, 11)\n",
            "Columns: ['hour_of_day', 'cash_type', 'money', 'coffee_name', 'Time_of_Day', 'Weekday', 'Month_name', 'Weekdaysort', 'Monthsort', 'Date', 'Time']\n",
            " hour_of_day cash_type  money   coffee_name Time_of_Day Weekday Month_name  Weekdaysort  Monthsort       Date    Time\n",
            "          10      card   38.7         Latte     Morning     Fri        Mar            5          3 01-03-2024 15:50.5\n",
            "          12      card   38.7 Hot Chocolate   Afternoon     Fri        Mar            5          3 01-03-2024 19:22.5\n",
            "          12      card   38.7 Hot Chocolate   Afternoon     Fri        Mar            5          3 01-03-2024 20:18.1\n",
            "          13      card   28.9     Americano   Afternoon     Fri        Mar            5          3 01-03-2024 46:33.0\n",
            "          13      card   38.7         Latte   Afternoon     Fri        Mar            5          3 01-03-2024 48:14.6\n",
            "Candidate item columns: ['coffee_name', 'Month_name']\n",
            "Candidate transaction-id columns: []\n",
            "Using item column: coffee_name\n",
            "Number of transactions: 3547\n",
            "Coffee-positive transactions: 1372 / 3547\n",
            "Distinct items: 8\n",
            "Candidate numeric cols for sale amount: ['money']\n",
            "Candidate hour/time cols: ['hour_of_day', 'Time']\n",
            "Candidate categorical cols (cashier/type): ['cash_type']\n",
            "Train shape: (2660, 9) Test shape: (887, 9)\n",
            "Positive rate (train): 0.3868421052631579  (test): 0.38669673055242393\n",
            "\n",
            "Training: LogisticRegression\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       544\n",
            "           1       1.00      1.00      1.00       343\n",
            "\n",
            "    accuracy                           1.00       887\n",
            "   macro avg       1.00      1.00      1.00       887\n",
            "weighted avg       1.00      1.00      1.00       887\n",
            "\n",
            "\n",
            "Training: RandomForest\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       544\n",
            "           1       1.00      1.00      1.00       343\n",
            "\n",
            "    accuracy                           1.00       887\n",
            "   macro avg       1.00      1.00      1.00       887\n",
            "weighted avg       1.00      1.00      1.00       887\n",
            "\n",
            "\n",
            "Training: GradientBoosting\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       544\n",
            "           1       1.00      1.00      1.00       343\n",
            "\n",
            "    accuracy                           1.00       887\n",
            "   macro avg       1.00      1.00      1.00       887\n",
            "weighted avg       1.00      1.00      1.00       887\n",
            "\n",
            "\n",
            "Training: BernoulliNB\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       544\n",
            "           1       1.00      1.00      1.00       343\n",
            "\n",
            "    accuracy                           1.00       887\n",
            "   macro avg       1.00      1.00      1.00       887\n",
            "weighted avg       1.00      1.00      1.00       887\n",
            "\n",
            "\n",
            "Training: KNeighbors\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       544\n",
            "           1       1.00      1.00      1.00       343\n",
            "\n",
            "    accuracy                           1.00       887\n",
            "   macro avg       1.00      1.00      1.00       887\n",
            "weighted avg       1.00      1.00      1.00       887\n",
            "\n",
            "\n",
            "Training: SVC\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       544\n",
            "           1       1.00      1.00      1.00       343\n",
            "\n",
            "    accuracy                           1.00       887\n",
            "   macro avg       1.00      1.00      1.00       887\n",
            "weighted avg       1.00      1.00      1.00       887\n",
            "\n",
            "\n",
            "Training: DecisionTree\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       544\n",
            "           1       1.00      1.00      1.00       343\n",
            "\n",
            "    accuracy                           1.00       887\n",
            "   macro avg       1.00      1.00      1.00       887\n",
            "weighted avg       1.00      1.00      1.00       887\n",
            "\n",
            "\n",
            "Saved model summary to: /mnt/data/coffee_classification_output/classification_summary.csv\n",
            "\n",
            "Outputs written to: /mnt/data/coffee_classification_output\n",
            "If you'd like: I can modify the target, add cross-validation, or produce ROC/PR curves.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MultiLabelBinarizer, OneHotEncoder, StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import joblib\n",
        "\n",
        "# -----------------------------\n",
        "# Config\n",
        "# -----------------------------\n",
        "DATA_PATH = \"/content/data_set.csv\"\n",
        "OUT_DIR = \"/mnt/data/coffee_classification_output\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "os.makedirs(os.path.join(OUT_DIR, \"classification_reports\"), exist_ok=True)\n",
        "os.makedirs(os.path.join(OUT_DIR, \"models\"), exist_ok=True)\n",
        "\n",
        "# Keywords used to mark a transaction as \"coffee\"\n",
        "COFFEE_KEYWORDS = [\n",
        "    \"coffee\", \"kopi\", \"instant coffee\", \"instant\", \"nescafe\", \"nescaf\", \"espresso\",\n",
        "    \"latte\", \"cappuccino\", \"mocha\", \"kopiko\", \"good day kopi\", \"good day kopi 3 in 1\",\n",
        "    \"white coffee\", \"kopi instan\", \"kopi sachet\"\n",
        "]\n",
        "\n",
        "# -----------------------------\n",
        "# Load dataset (robust)\n",
        "# -----------------------------\n",
        "def load_csv(path):\n",
        "    for enc in (\"utf-8\", \"latin1\", \"cp1252\"):\n",
        "        try:\n",
        "            return pd.read_csv(path, encoding=enc, low_memory=False)\n",
        "        except Exception:\n",
        "            continue\n",
        "    raise RuntimeError(f\"Unable to read CSV at {path}\")\n",
        "\n",
        "print(\"Loading dataset...\", DATA_PATH)\n",
        "df = load_csv(DATA_PATH)\n",
        "print(\"Dataset shape:\", df.shape)\n",
        "print(\"Columns:\", list(df.columns))\n",
        "\n",
        "# Show a quick preview (comment out if running non-interactively)\n",
        "print(df.head(5).to_string(index=False))\n",
        "\n",
        "# -----------------------------\n",
        "# Find item / transaction columns\n",
        "# -----------------------------\n",
        "# Prefer explicit columns that commonly appear in retail transaction datasets\n",
        "possible_item_cols = [c for c in df.columns if 'item' in c.lower() or 'product' in c.lower() or 'coffee' in c.lower() or 'name' in c.lower()]\n",
        "possible_tid_cols = [c for c in df.columns if 'tid' in c.lower() or 'trans' in c.lower() or 'receipt' in c.lower() or 'id' == c.lower()]\n",
        "\n",
        "print(\"Candidate item columns:\", possible_item_cols)\n",
        "print(\"Candidate transaction-id columns:\", possible_tid_cols)\n",
        "\n",
        "# Choose item column heuristically\n",
        "if 'coffee_name' in df.columns:\n",
        "    item_col = 'coffee_name'\n",
        "elif possible_item_cols:\n",
        "    item_col = possible_item_cols[0]\n",
        "else:\n",
        "    # fallback: try 'Items' or 'items'\n",
        "    item_col = None\n",
        "    for fallback in ['Items', 'items', 'Item', 'item']:\n",
        "        if fallback in df.columns:\n",
        "            item_col = fallback\n",
        "            break\n",
        "    if item_col is None:\n",
        "        raise RuntimeError(\"Couldn't detect an item column. Please ensure the CSV has an item/product column.\")\n",
        "\n",
        "print(\"Using item column:\", item_col)\n",
        "\n",
        "# If there's a transaction id and multiple rows per transaction, group into lists\n",
        "if possible_tid_cols:\n",
        "    tid_col = possible_tid_cols[0]\n",
        "    # Check if multiple rows per tid\n",
        "    if df.groupby(tid_col).size().max() > 1:\n",
        "        baskets = df.groupby(tid_col)[item_col].apply(lambda s: s.astype(str).str.strip().tolist()).reset_index()\n",
        "        baskets.columns = [tid_col, 'items']\n",
        "    else:\n",
        "        # Assume item column is comma-separated list per row\n",
        "        baskets = df[[item_col]].copy()\n",
        "        baskets['items'] = baskets[item_col].astype(str).apply(\n",
        "            lambda x: [it.strip() for it in str(x).split(',') if it.strip()!='']\n",
        "        )\n",
        "        baskets['TID'] = baskets.index.astype(str)\n",
        "        tid_col = 'TID'\n",
        "        baskets = baskets[[tid_col, 'items']]\n",
        "else:\n",
        "    # No tid col -> assume item list per row\n",
        "    baskets = df[[item_col]].copy()\n",
        "    baskets.columns = ['items_raw']\n",
        "    baskets['items'] = baskets['items_raw'].astype(str).apply(\n",
        "        lambda x: [it.strip() for it in str(x).split(',') if it.strip()!='']\n",
        "    )\n",
        "    baskets['TID'] = baskets.index.astype(str)\n",
        "    tid_col = 'TID'\n",
        "    baskets = baskets[[tid_col, 'items']]\n",
        "\n",
        "print(\"Number of transactions:\", len(baskets))\n",
        "\n",
        "# Normalize item strings\n",
        "def normalize_items(lst):\n",
        "    return [str(x).strip().lower() for x in lst if str(x).strip()!='']\n",
        "\n",
        "baskets['items_norm'] = baskets['items'].apply(normalize_items)\n",
        "\n",
        "# -----------------------------\n",
        "# Create target: contains_coffee\n",
        "# -----------------------------\n",
        "def contains_coffee(tokens):\n",
        "    for it in tokens:\n",
        "        for kw in COFFEE_KEYWORDS:\n",
        "            if kw in it:\n",
        "                return 1\n",
        "    return 0\n",
        "\n",
        "baskets['contains_coffee'] = baskets['items_norm'].apply(contains_coffee)\n",
        "print(\"Coffee-positive transactions:\", baskets['contains_coffee'].sum(), \"/\", len(baskets))\n",
        "\n",
        "# -----------------------------\n",
        "# Create one-hot item features (top-K) to limit dimensionality\n",
        "# -----------------------------\n",
        "mlb = MultiLabelBinarizer(sparse_output=False)\n",
        "X_items = mlb.fit_transform(baskets['items_norm'])\n",
        "item_features = mlb.classes_.tolist()\n",
        "print(\"Distinct items:\", len(item_features))\n",
        "\n",
        "# Optionally reduce to most frequent K features to avoid huge sparse matrix\n",
        "MAX_ITEM_FEATURES = 1000\n",
        "if X_items.shape[1] > MAX_ITEM_FEATURES:\n",
        "    freqs = X_items.sum(axis=0)\n",
        "    top_idx = np.argsort(freqs)[-MAX_ITEM_FEATURES:]\n",
        "    X_items = X_items[:, top_idx]\n",
        "    item_features = [item_features[i] for i in top_idx]\n",
        "    print(f\"Reduced item features to top {MAX_ITEM_FEATURES}\")\n",
        "\n",
        "X = X_items\n",
        "feature_names = item_features.copy()\n",
        "\n",
        "# -----------------------------\n",
        "# Add extra features if present in original CSV (hour_of_day, money, cashier type)\n",
        "# -----------------------------\n",
        "# We'll try to pull those from the original df by matching on index or tid where possible.\n",
        "extra_df = None\n",
        "if 'TID' in df.columns and tid_col in df.columns and 'TID' == tid_col:\n",
        "    extra_df = df\n",
        "else:\n",
        "    # Try to join using transaction id column if it exists\n",
        "    if tid_col in df.columns:\n",
        "        extra_df = df\n",
        "    else:\n",
        "        extra_df = df  # fallback; we will try to extract columns by position\n",
        "\n",
        "# Attempt to attach 'money' or 'amount' if present\n",
        "candidate_numeric = [c for c in df.columns if any(k in c.lower() for k in ['money','amount','price','total','value'])]\n",
        "candidate_hour = [c for c in df.columns if 'hour' in c.lower() or 'time'==c.lower()]\n",
        "candidate_cat = [c for c in df.columns if 'cash' in c.lower() or 'cashier' in c.lower() or 'type' in c.lower()]\n",
        "\n",
        "print(\"Candidate numeric cols for sale amount:\", candidate_numeric)\n",
        "print(\"Candidate hour/time cols:\", candidate_hour)\n",
        "print(\"Candidate categorical cols (cashier/type):\", candidate_cat)\n",
        "\n",
        "# We'll try to extract columns from df by using the same row order if there's no explicit tid mapping.\n",
        "# Build aux features using aggregation: if multiple rows per tid, compute mean money per tid; otherwise map values directly.\n",
        "aux = pd.DataFrame({tid_col: baskets[tid_col].values})\n",
        "aux.set_index(tid_col, inplace=True)\n",
        "\n",
        "# If 'money' like column exists, aggregate by tid from original df if possible\n",
        "if candidate_numeric:\n",
        "    money_col = candidate_numeric[0]\n",
        "    try:\n",
        "        if tid_col in df.columns:\n",
        "            money_agg = df.groupby(tid_col)[money_col].agg('mean').astype(float)\n",
        "            aux = aux.join(money_agg, how='left')\n",
        "            feature_names.append(money_col)\n",
        "        else:\n",
        "            # approximate: try to take first N rows corresponding to baskets order\n",
        "            aux[money_col] = np.nan\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "# If hour/time exists\n",
        "if candidate_hour:\n",
        "    hour_col = candidate_hour[0]\n",
        "    try:\n",
        "        if tid_col in df.columns:\n",
        "            hour_agg = df.groupby(tid_col)[hour_col].agg('first')\n",
        "            aux = aux.join(hour_agg, how='left')\n",
        "            feature_names.append(hour_col)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "# If cashier/type exists (categorical), one-hot encode and join\n",
        "if candidate_cat:\n",
        "    cat_col = candidate_cat[0]\n",
        "    try:\n",
        "        if tid_col in df.columns:\n",
        "            cat_agg = df.groupby(tid_col)[cat_col].agg('first').astype(str)\n",
        "            # one-hot encode\n",
        "            ohe = pd.get_dummies(cat_agg, prefix=cat_col)\n",
        "            aux = aux.join(ohe, how='left')\n",
        "            feature_names.extend(ohe.columns.tolist())\n",
        "        else:\n",
        "            pass\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "# Fill missing numeric values with 0 and fill NaN for dummies with 0\n",
        "aux = aux.fillna(0)\n",
        "if len(aux.columns) > 0:\n",
        "    # Concatenate item features + aux numeric features\n",
        "    X = np.hstack([X, aux.values])\n",
        "    # extend feature_names already done above\n",
        "else:\n",
        "    # no extra features added\n",
        "    pass\n",
        "\n",
        "y = baskets['contains_coffee'].values\n",
        "\n",
        "# -----------------------------\n",
        "# Train/test split\n",
        "# -----------------------------\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(\"Train shape:\", X_train.shape, \"Test shape:\", X_test.shape)\n",
        "print(\"Positive rate (train):\", y_train.mean(), \" (test):\", y_test.mean())\n",
        "\n",
        "# -----------------------------\n",
        "# Classifiers to train\n",
        "# -----------------------------\n",
        "models = {\n",
        "    \"LogisticRegression\": make_pipeline(StandardScaler(with_mean=False), LogisticRegression(max_iter=1000, solver='liblinear')),\n",
        "    \"RandomForest\": RandomForestClassifier(n_estimators=200, random_state=42),\n",
        "    \"GradientBoosting\": GradientBoostingClassifier(random_state=42),\n",
        "    \"BernoulliNB\": BernoulliNB(),\n",
        "    \"KNeighbors\": KNeighborsClassifier(n_neighbors=5),\n",
        "    \"SVC\": make_pipeline(StandardScaler(with_mean=False), SVC(kernel='rbf', probability=True)),\n",
        "    \"DecisionTree\": DecisionTreeClassifier(random_state=42)\n",
        "}\n",
        "\n",
        "results = []\n",
        "for name, clf in models.items():\n",
        "    print(\"\\nTraining:\", name)\n",
        "    try:\n",
        "        clf.fit(X_train, y_train)\n",
        "        y_pred = clf.predict(X_test)\n",
        "        acc = accuracy_score(y_test, y_pred)\n",
        "        rep_text = classification_report(y_test, y_pred, zero_division=0)\n",
        "        print(rep_text)\n",
        "        # Save detailed report to file\n",
        "        with open(os.path.join(OUT_DIR, \"classification_reports\", f\"{name}_report.txt\"), \"w\") as f:\n",
        "            f.write(f\"Model: {name}\\n\\n\")\n",
        "            f.write(\"Classification report:\\n\")\n",
        "            f.write(rep_text + \"\\n\")\n",
        "            f.write(f\"Accuracy: {acc}\\n\")\n",
        "        # Save model\n",
        "        joblib.dump(clf, os.path.join(OUT_DIR, \"models\", f\"{name}.joblib\"))\n",
        "        # Summarize positive class metrics\n",
        "        rep_dict = {}\n",
        "        try:\n",
        "            # use sklearn's classification_report dict form\n",
        "            from sklearn.metrics import precision_recall_fscore_support\n",
        "            p, r, f, s = precision_recall_fscore_support(y_test, y_pred, zero_division=0)\n",
        "            # positive class is index 1 assuming binary 0/1\n",
        "            pos_idx = 1\n",
        "            precision_pos = p[pos_idx] if pos_idx < len(p) else 0.0\n",
        "            recall_pos = r[pos_idx] if pos_idx < len(r) else 0.0\n",
        "            f1_pos = f[pos_idx] if pos_idx < len(f) else 0.0\n",
        "        except Exception:\n",
        "            precision_pos = recall_pos = f1_pos = None\n",
        "        results.append({\n",
        "            \"model\": name,\n",
        "            \"accuracy\": acc,\n",
        "            \"precision_pos\": precision_pos,\n",
        "            \"recall_pos\": recall_pos,\n",
        "            \"f1_pos\": f1_pos\n",
        "        })\n",
        "    except Exception as e:\n",
        "        print(f\"Model {name} failed: {e}\")\n",
        "\n",
        "# Save summary CSV\n",
        "summary_df = pd.DataFrame(results).sort_values(by='f1_pos', ascending=False)\n",
        "summary_csv = os.path.join(OUT_DIR, \"classification_summary.csv\")\n",
        "summary_df.to_csv(summary_csv, index=False)\n",
        "print(\"\\nSaved model summary to:\", summary_csv)\n",
        "\n",
        "print(\"\\nOutputs written to:\", OUT_DIR)\n",
        "print(\"If you'd like: I can modify the target, add cross-validation, or produce ROC/PR curves.\")\n"
      ]
    }
  ]
}